{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Create a RandomForest classifier\nclf = RandomForestClassifier(random_state=42)\n\n# K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n\nprint(\"K-Fold Cross-Validation Scores:\", scores)\nprint(\"Mean Accuracy:\", np.mean(scores))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T05:10:02.377119Z","iopub.execute_input":"2026-01-05T05:10:02.377573Z","iopub.status.idle":"2026-01-05T05:10:05.751120Z","shell.execute_reply.started":"2026-01-05T05:10:02.377540Z","shell.execute_reply":"2026-01-05T05:10:05.750091Z"}},"outputs":[{"name":"stdout","text":"K-Fold Cross-Validation Scores: [1.         0.96666667 0.93333333 0.93333333 0.96666667]\nMean Accuracy: 0.9600000000000002\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Cross-Validation Techniques\n\n\nCross-validation is a crucial method in machine learning that helps evaluate and improve the performance of a model. It ensures that the model generalizes well to unseen data and helps in hyperparameter tuning to reduce overfitting and achieve low variance. This article will discuss the importance of cross-validation, its benefits, and various cross-validation techniques used in machine learning.\n\n#### Why is Cross-Validation Important?\n\nIn machine learning, our goal is to build a model that performs well on new, unseen data, not just on the training data. Cross-validation helps in:\n\nGeneralization: Ensuring that the model performs well on different datasets.\nReducing Overfitting: Preventing the model from being too specific to the training data.\nLow Variance: Achieving consistent performance across different datasets.\nBasic Concept\nTypically, we split our dataset into training and testing sets. For instance, with a dataset of 1000 records, we might use 80% (800 records) for training and 20% (200 records) for testing. However, a single train-test split might not be sufficient to evaluate the model reliably. This is where cross-validation comes in.\n\n#### Types of Cross-Validation Techniques: \n\n1. Leave-One-Out Cross-Validation (LOOCV)\nIn LOOCV, each datapoint is used once as a validation set while the remaining points form the training set. This process is repeated for all data points.\n\nFor a dataset with n points, train on n−1 points and test on the remaining one.\nRepeat for all data points and average the results.\nExample: With 5 records, each record will be the validation set in 5 different iterations.\n\n2. Hold-Out Cross-Validation\nThe dataset is split into two parts: a training set and a validation set. Typically, 80% of the data is used for training and 20% for validation.\n\nTrain the model on the training set and validate it on the validation set.\nThis method is simple but may not provide a reliable performance estimate.\nExample: With 800 training records, split into 80% training and 20% validation repeatedly.\n\n3. K-Fold Cross-Validation\nThe dataset is divided into 'k' subsets (folds). The model is trained on k−1 folds and tested on the remaining fold. This process is repeated 'k' times.\n\nFor example, with k=10, the dataset is split into 10 folds, each containing 100 records. In each iteration, one fold is used for validation, and the remaining 9 folds are used for training.\n\nAverage the performance metrics across all folds.\n4. Stratified K-Fold Cross-Validation\nSimilar to K-Fold, but ensures each fold has the same proportion of class labels as the original dataset.\n\nParticularly useful for imbalanced datasets.\nEnsures class proportions are maintained in each fold, balancing 0 and 1 classes in a binary classification problem.\nPractical Implementation of K-Fold Cross-Validation\n\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n​\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n​\n# Create a RandomForest classifier\nclf = RandomForestClassifier(random_state=42)\n​\n# K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n​\nprint(\"K-Fold Cross-Validation Scores:\", scores)\nprint(\"Mean Accuracy:\", np.mean(scores))\n\nK-Fold Cross-Validation Scores: [1.         0.96666667 0.93333333 0.93333333 0.96666667]\nMean Accuracy: 0.9600000000000002\n\n### Conclusion\nCross-validation is essential for evaluating and improving machine learning models. It ensures that models generalize well to unseen data and helps in tuning hyperparameters to reduce overfitting. Various techniques like LOOCV, Hold-Out, K-Fold, and Stratified K-Fold provide different ways to achieve reliable model performance.","metadata":{}}]}